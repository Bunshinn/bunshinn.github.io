
[TOC]

PART ONE: 定义, 预处理, 步骤, 评估
=================================

## 请简要说说一个完整机器学习项目的流程  
1 抽象成数学问题  
2 获取数据  
3 特征预处理与特征选择  
4 训练模型与调优  
5 模型诊断  
6 模型融合  
7 上线运行  

## 数据预处理过程有哪些？  
    1）缺失值处理：删、插  
    2）异常值处理  
    3）特征转换：时间特征sin化表示  
    4）标准化：最大最小标准化、z标准化等  
    5）归一化：对于文本或评分特征，不同样本之间可能有整体上的差异，如a文本共20个词，b文本30000个词，b文本中各个维度上的频次都很可能远远高于a文本  
    6）离散化：onehot、分箱等  

## 怎么处理数据中的离群值  
离群值的存在会影响到对数据的拟合和预测，通常需要加以处理，大致可以分为两类方法  
第一类的方法可以参考缺失值处理：1）直接删除 2）替换：可以使用均值、中位数、众数进行替换  
第二类的方法是离群值处理特有的：利用拉依达准则法（3σ准则），将超出这个范围的值替换成设定的阈值，通常为均值±3σ、均值±2σ，视情况而定  

## 缺失值处理方法   
数据清理中，处理缺失值的方法有两种：   
删除法:  
 1）删除观察样本  
 2）删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除  
 3）使用完整原始数据分析：当数据存在较多缺失而其原始数据完整时，可以使用原始数据替代现有数据进行分析  
 4）改变权重：当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加权，可以降低删除缺失数据带来的偏差
插补法:
 1）用中位数、平均值、众数等填充  
 2）插补：同类均值插补、多重插补、极大似然估计, 抽样填补  
 3）用其它字段构建模型，预测该字段的值，从而填充缺失值, 比如:回归插补（注意：如果该字段也是用于预测模型中作为特征，那么用其它字段建模填充缺失值的方式，并没有给最终的预测模型引入新信息）  
 4）onehot，将缺失值也认为一种取值  
 5）压缩感知及矩阵补全  
查补法：均值插补、回归插补、抽样填补等  
成对删除与改变权重为一类  
估算与查补法为一类  

## 数据不平衡问题  
1.采样，对小样本加噪声采样，对大样本进行下采样  
2.数据生成，利用已知样本生成新的样本  
3.进行特殊的加权，如在Adaboost中或者SVM中  
4.采用对不平衡数据集不敏感的算法  
5.改变评价标准：用AUC/ROC来进行评价  
6.采用Bagging/Boosting/ensemble等方法  
7.在设计模型的时候考虑数据的先验分布  

## 谈谈判别式模型和生成式模型  
判别方法：由数据直接学习决策函数Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型    
生成方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型  
由生成模型可以得到判别模型，但由判别模型得不到生成模型  
常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场  
常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）  

## 监督学习和无监督学习有什么区别  
监督学习：对具有标记（分类）的训练样本进行学习，这里，所有的标记（分类）是已知的。 如：决策树算法、朴素贝叶斯算法、 KNN 算法等  
无监督学习：对没有标记（分类）的训练样本进行学习，目的是为了发现训练集中的结构特性。这里，所有的标记（分类）是未知的。 如：聚类算法  

## 无监督和有监督算法的区别？

监督学习（supervised learning）：通过已有的训练样本（即已知数据以及其对应的输出）来训练，从而得到一个最优模型，再利用这个模型将所有新的数据样本映射为相应的输出结果，对输出结果进行简单的判断从而实现分类的目的，那么这个最优模型也就具有了对未知数据进行分类的能力。包括所有的回归算法分类算法，比如线性回归、决策树、KNN、SVM等
无监督学习（unsupervised learning）：我们事先没有任何训练数据样本，需要直接对数据进行建模。包括所有的聚类算法，比如k-means 、PCA、 GMM等


## 常见分类、聚类、回归算法有区别以及适用场景

## 机器学习常见评估指标  
分类问题评估指标:

* Accuracy
* Precision
* Recall
* F1-Score
* ROC-curve
* PR-curve
* AUC

回归问题评估指标:

* 相对绝对误差(AE,RE)  
* 平均绝对误差（MAE）  
* 根均方差(MSE)   
* 均方根误差(RMSE)

## 分类算法性能的主要评价指标  

    1）查准率、查全率、F1  
    2）AUC  
    3）LOSS  
    4）Gain和Lift  
    5）WOE和IV  

## 有关分类算法的准确率，召回率，F1值  
对于二类分类问题常用的评价指标是精准度（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不
正确，4 种情况出现的总数分别记作：  
  TP——将正类预测为正类数  
  FN——将正类预测为负类数  
  FP——将负类预测为正类数  
  TN——将负类预测为负类数  
由此：  
  精准率定义为：P = TP / (TP + FP)  
  召回率定义为：R = TP / (TP + FN)  
  F1 值定义为： F1 = 2 P R / (P + R)  


## 请解释过拟合，以及如何防止过度拟合  
过拟合：是指为了得到一致假设而使假设变得过度严格判断过拟合的方法：一个假设（模型）在训练数据上能够获得比其他假设（模型）  
更好的拟合， 但是在【训练数据外】 的数据集上却不能很好地拟合数据，这就意味着出现了过拟合现象。  
解决方法有：增大数据量、交叉验证、正则化特征、减少特征、减少权值  

## 为什么会产生过拟合，有哪些方法可以预防或克服过拟合？(2次)

所谓过拟合（Overfit），是这样一种现象：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好的拟合数据。此时我们就叫这个假设出现了overfit的现象。
过拟合产生的原因：出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少。
解决方法：
1、 增大数据量
2、 减少feature个数（人工定义留多少个feature或者算法选取这些feature）
3、 正则化（留下所有的feature，但对于部分feature定义其parameter非常小）
4、 交叉验证，重采样评价模型效能，K折交叉验证
5、保留一个验证数据集检验

## 过拟合的解决办法  
    1）增加数据   
    2）正则项  
    3）early stopping  
    4）控制模型复杂度：  
        a. dropout（我觉得类似于subfeature）  
        b. 剪枝、控制树深  
        c. 增大分割平面间隔  
    5）bagging  
    6）subsampe & subfeature  
    7）特征选择、特征降维  
    8）数据增强（加包含噪声的数据）  
    9）ensemble  
（参考林轩田的《机器学习技法》）  

## 正则化  
正则化是针对过拟合而提出的，因为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。  
L2正则化：目标函数中增加所有权重w参数的平方之和, 逼迫所有w尽可能趋向零但不为零. 因为过拟合的时候, 拟合函数需要顾忌每一个点, 最终形成的拟合函数波动很大, 在某些很小的区间里, 函数值的变化很剧烈, 也就是某些w非常大. 为此, L2正则化的加入就惩罚了权重变大的趋势  
L1正则化：目标函数中增加所有权重w参数的绝对值之和, 逼迫更多w为零(也就是变稀疏. L2因为其导数也趋0, 奔向零的速度不如L1给力了)  

## 为什么正则化能处理过拟合  
1）惩罚了模型的复杂度，避免模型过度学习训练集，提高泛化能力  
2）剃刀原理：如果两个理论都能解释一件事情，那么较为简单的理论往往是正确的  
3）正则项降低了每一次系数w更新的步伐，使参数更小，模型更简单  
4）贝叶斯学派的观点，认为加入了先验分布（l1拉普拉斯分布，l2高斯分布），减少参数的选择空间  


## 机器学习中，为何要经常对数据做归一化  
1）归一化后加快了梯度下降求最优解的速度  
2）归一化有可能提高精度

## 简单说下sigmoid激活函数  
非线性激活函数，相当于把一个实数压缩至0到1之间

## 什么是卷积  
对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作  


PART TWO: 各模型主要思想, 优缺点
================================

## KNN
### KNN思想

### 一般，K-NN 最近邻方法在什么情况下效果较好  
knn针对，整体样本应该具有典型性好，样本较少，比较适宜。便于发挥出其求近邻的优势，若样本呈现团状分布就无法计算近邻。




## DR(Decision Tree)

### 如何避免决策树过拟合  
   1）限制树深  
   2）剪枝  
   3）限制叶节点数量  
   4）正则化项  
   5）增加数据  
   6）bagging（subsample、subfeature、低维空间投影）  
   7）数据增强（加入有杂质的数据）  
   8）早停  


###C4.5决策树  
C4.5算法的优点是：产生的分类规则易于理解，不用做特征选择，准确率较高。  
C4.5算法的缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效  
C4.5对缺失值不敏感，因为它有应对缺失值的处理方案。  

## NB(Naive Bayes)
 
### 朴素贝叶斯的理解  
   1）推导（[参考：](https://www.cnblogs.com/pinard/p/6069267.html)）  
   2）理解：朴素贝叶斯是在已知一些先验概率的情况下，由果索因的一种方法  
   3）其它：朴素的意思是假设了事件相互独立  


## LR(logist Regression)
  
### 15. LR  
    1）用于分类问题的线性回归  
    2）采用sigmoid对输出值进行01转换  
    3）采用似然法求解  
    4）手推  
    5）优缺点局限性  
    6）改进空间  
  

## K-MEANS

### Kmeans的原理  
   1）初始化k个点  
   2）根据距离点归入k个类中  
   3）更新k个类的类中心  
   4）重复（2）（3），直到收敛或达到迭代次数  

### 什么是聚类分析？聚类算法有哪几种？请选择一种详细描述其计算原理和步骤  
  
1）聚类分析是一种无监督的学习方法，根据一定条件将相对同质的样本归到一个类总（俗话说人以类聚，物以群分）  
 正式一点的：聚类是对点集进行考察并按照某种距离测度将他们聚成多个“簇”的过程。聚类的目标是使得同一簇内的点之间的距离较短，而不同簇中点之间的距离较大。  
2）聚类方法主要有：  
a. 层次聚类  
层次法（hierarchical methods），这种方法对给定的数据集进行层次似的分解，直到某种条件满足为止。。具体又可分为“自底向上”和“自顶向下”两种方案。  
　　例如，在“自底向上”方案中，初始时每一个数据纪录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。  
经典算法为：CURE；采用抽样技术先对数据集D随机抽取样本，再采用分区技术对样本进行分区，然后对每个分区局部聚类，最后对局部聚类进行全局聚类。  
b. 划分聚类：（经典算法为kmeans）  
划分法（parTITIoning methods），给定一个有N个元组或者纪录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K《N。而且这K个分组满足下列条件：  
　　（1） 每一个分组至少包含一个数据纪录；  
　　（2）每一个数据纪录属于且仅属于一个分组（注意：这个要求在某些模糊聚类算法中可以放宽）；  
　　对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准就是：同一分组中的记录越近越好，而不同分组中的纪录越远越好。  
c. 密度聚类  
基于密度的方法（density-based methods），基于密度的方法与其它方法的一个根本区别是：它不是基于各种各样的距离的，而是基于密度的。这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。  
经典算法：DBSCAN:DBSCAN算法是一种典型的基于密度的聚类算法，该算法采用空间索引技术来搜索对象的邻域，引入了“核心对象”和“密度可达”等概念，从核心对象出发，把所有密度可达的对象组成一个簇。  
这个方法的指导思想：只要一个区域中的点的密度大过某个阈值，就把它加到与之相近的聚类中去。  
d. 网格聚类  
基于网格的方法（grid-based methods），这种方法首先将数据空间划分成为有限个单元（cell）的网格结构，所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关的，它只与把数据空间分为多少个单元有关。  
经典算法：STING：利用网格单元保存数据统计信息，从而实现多分辨率的聚类  
e. 模型聚类：高斯混合模型  
基于模型的方法（model-based methods），基于模型的方法给每一个聚类假定一个模型，然后去寻找能够很好的满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。  
3）k-means比较好介绍，选k个点开始作为聚类中心，然后剩下的点根据距离划分到类中；找到新的类中心；重新分配点；迭代直到达到收敛条件或者迭代次数。 优点是快；缺点是要先指定k，同时对异常值很敏感。  

### 简述k-means聚类的基本思想、步骤以及k-means的缺点  
k-means的基本思想：  
通过迭代寻找k个聚类的一种划分方案，使得用这k个聚类的均值来代表相应各类样本时所得的总体误差最小。  
k-means算法的基础是最小误差平方和准则  

K-means 聚类步骤： 
Step1: 随机选择 k 个质心（即 k 个类）  
Step2: 计算每一个点到这些质心的距离，然后决定每个点所属的类    
Step3: 对于每个类，重新确定该类的质心  
Step4: 若收敛，则结束；否则转到 Step2  

K-means缺点：
1.对聚类中心的初始化比较敏感，不同的初始化带来不同的聚类结果  
2.K值需要首先人工确定(启发式)  
3.只能处理服从标准正太分布的聚类  
4.K-means对于噪声比较敏感  


### 选个讲下原理吧 K-Means算法及改进，遇到异常值怎么办？评估算法的指标有哪些？  
    1）k-means原理  
    2）改进：  
        a. kmeans++：初始随机点选择尽可能远，避免陷入局部解。方法是n+1个中心点选择时，对于离前n个点选择到的概率更大  
        b. mini batch kmeans：每次只用一个子集做重入类并找到类心（提高训练速度）  
        c. ISODATA：对于难以确定k的时候，使用该方法。思路是当类下的样本小时，剔除；类下样本数量多时，拆分  
        d. kernel kmeans：kmeans用欧氏距离计算相似度，也可以使用kernel映射到高维空间再聚类  
    3）遇到异常值  
        a. 有条件的话使用密度聚类或者一些软聚类的方式先聚类, 剔除异常值。不过本来用kmeans就是为了快, 这么做有些南辕北辙了  
        b. 局部异常因子LOF：如果点p的密度明显小于其邻域点的密度，那么点p可能是异常值（[参考](https://blog.csdn.net/wangyibo0201/article/details/51705966)）  
        c. 多元高斯分布异常点检测  
        d. 使用PCA或自动编码机进行异常点检测：使用降维后的维度作为新的特征空间，其降维结果可以认为剔除了异常值的影响（因为过程是保留使投影后方差最大的投影方向）  
        e. isolation forest：基本思路是建立树模型，一个节点所在的树深度越低，说明将其从样本空间划分出去越容易，因此越可能是异常值。是一种无监督的方法，随机选择n个sumsampe，随机选择一个特征一个值。（[参考](https://blog.csdn.net/u013709270/article/details/73436588)）  
        f. winsorize：对于简单的，可以对单一维度做上下截取  
    4）评估聚类算法的指标：  
        a. 外部法（基于有标注）：Jaccard系数、纯度  
        b. 内部法（无标注）：内平方和WSS和外平方和BSS  
        c. 此外还要考虑到算法的时间空间复杂度、聚类稳定性等  

### 在K-Means中如何拾取k  
K-Means 算法的最大缺点是不能自动选择分类数k，常见的确定k的方法有:  
- 根据先验知识来确定  
- k=sqrt(N/2) N为样本数  
- 拐点法：把聚类结果的F-test值对聚类个数的曲线画出来，选择图中的拐点  
- 基于信息准则判断，如果模型有似然函数，则可以用 BIC、 DIC 来进行决策  
具体的 k 的选择往往和业务联系紧密，如希望能将用户进行分类，就有先验的分类要求


## SVM

## 简要介绍下svm  
SVM，全称是support vector machine，中文名叫支持向量机。SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开
[原理推导](http://blog.csdn.net/v_july_v/article/details/7624837)

2.简要介绍下tensorflow的计算图  
Tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个节点都是计算图上的一个Tensor, 也就是张量，而节点之间的边描述了计算之间的依赖关系(定义时)和数学操作(运算时)

### SVM的优点  
   1）优点：  
      a. 能应用于非线性可分的情况  
      b. 最后分类时由支持向量决定，复杂度取决于支持向量的数目而不是样本空间的维度，避免了维度灾难  
      c. 具有鲁棒性：因为只使用少量支持向量，抓住关键样本，剔除冗余样本  
      d. 高维低样本下性能好，如文本分类  
   2）缺点：  
      a. 模型训练复杂度高  
      b. 难以适应多分类问题  
      c. 核函数选择没有较好的方法论  

## RF(Random Forest)

### 随机森林原理？有哪些随机方法？  
    1）随机森林原理：通过构造多个决策树，做bagging以提高泛化能力  
    2）subsample（有放回抽样）、subfeature、低维空间投影（特征做组合，参考林轩田的《机器学习基石》）  

### 简单的介绍随机森林，以及一些细节  
    1）随机森林原理：通过构造多个决策树，做bagging以提高泛化能力  
    2）随机方法包括：subsample（有放回抽样）、subfeature、低维空间投影（特征做组合，参考林轩田的《机器学习基石》）  
    3）有放回抽样，可以用包外样本做检验  
    4）也可以用OOB做特征选择，思路：  
        a. 如果一个特征有效，那么这个特征引入杂质会明显影响模型效果  
        b. 引入杂质会影响分布，所以更好的方式是对特征中的取值进行洗牌，然后计算前后模型的差异  
        c. 但是我们不想训练两个模型，可以利用OOB进行偷懒。把OOB中的数据该特征取值洗牌，然后扔进训练好的模型中，用输出的结果进行误差检验  

## XGBoost

### 为什么xgboost要用泰勒展开，优势在哪里？  
xgboost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归  

### xgboost如何寻找最优特征？是又放回还是无放回的呢？  
xgboost在训练的过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据, 从而记忆了每个特征对在模型训练时的重要性 -- 从根到叶子中间节点涉及某特征的次数作为该特征重要性排序.  
xgboost属于boosting集成学习方法, 样本是不放回的, 因而每轮计算样本不重复. 另一方面, xgboost支持子采样, 也就是每轮计算可以不使用全部样本, 以减少过拟合. 进一步地, xgboost 还有列采样, 每轮计算按百分比随机采样一部分特征, 既提高计算速度又减少过拟合  



## GBDT

### GBDT原理介绍下  
    1）首先介绍Adaboost Tree，是一种boosting的树集成方法。基本思路是依次训练多棵树，每棵树训练时对分错的样本进行加权。树模型中对样本的加权实际是对样本采样几率的加权，在进行有放回抽样时，分错的样本更有可能被抽到  
    2）GBDT是Adaboost Tree的改进，每棵树都是CART（分类回归树），树在叶节点输出的是一个数值，分类误差就是真实值减去叶节点的输出值，得到残差。GBDT要做的就是使用梯度下降的方法减少分类误差值  
    在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。  
GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。[参考：](https://www.cnblogs.com/pinard/p/6140514.html)  
    3）得到多棵树后，根据每颗树的分类误差进行加权投票  





PART THREE: 模型间对比
=====================

## plsa和lda  
LDA是pLSA的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词语分布，只是用的参数推断方法不同，在pLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两参数弄成随机变量，且加入dirichlet先验。dirichlet先验为某篇文档随机抽取出某个主题分布和词分布。plsa认为主题分布和词分布是唯一确定的  

## LR和SVM的联系与区别  
1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 
2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的  
区别： 
1、LR是参数模型，SVM是非参数模型  
2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重  
3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重  
4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算  
5、logic能做的svm能做，但可能在准确率上有问题，svm能做的logic有的做不了  

## LR与线性回归的区别与联系  
逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题

## 线性回归和逻辑回归的区别  
  
线性回归针对的目标变量是区间型的，   
逻辑回归针对的目标变量是类别型的  
线性回归模型的目标变量和自变量之间的关系假设是线性相关的 ，逻辑回归模型中的目标变量和自变量是非线性的  
线性回归中通常会用假设，对应于自变量x的某个值，目标变量y的观察值是服从正太分布的。逻辑回归中目标变量y是服从二项分布0和1或者多项分布的  
逻辑回归中不存在线性回归中常见的残差  
参数估值上，线性回归采用最小平方法，逻辑回归采用最大似染法。  

## bagging和boosting的区别  
Boosting和Bagging都是组合多个分类器投票的方法，二者均是根据单个分类器的正确率决定其权重。Bagging与Boosting的区别：取样方式不同  
Bagging采用均匀取样，而Boosting根据错误率取样。Bagging的各个预测函数没有权重，而Boosting是由权重的，Bagging的各个预测函数可以并行生成  
而Boosting的各个预测函数只能顺序生成  


## 决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么  
Bagging和Boosting属于集成学习的两类方法. Bagging方法有放回地采样同数量样本训练每个学习器, 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器, 迭代集成(平滑加权)  
决策树属于最常用的学习器, 其学习过程是从根建立树, 也就是如何决策叶子节点分裂. ID3/C4.5决策树用信息熵计算最优分裂, CART决策树用基尼指数计算最优分裂, xgboost决策树使用二阶泰勒展开系数计算最优分裂  

## 请说明随机森林较一般决策树稳定的几点原因  
  
随机森林分类的过程就是对于每个随机产生的决策树分类器，输入特征向量，森林中每棵决策树对样本进行分类，根据每个决策树的权重得到最后的分类结果。即随机森林就是由多颗决策树形成的并且随机森林是并行计算多颗决策树。  
bagging的方法，多个树投票提高泛化能力  
bagging中引入随机（参数、样本、特征、空间映射），避免单棵树的过拟合，提高整体泛化能力  
  
决策树缺点和注意事项：  
    决策树的最大缺点是原理中的贪心算法。因此它所做的选择只能是某种意义上的局部最优选择。  
    若目标变量是连续变量，那么决策树就不使用了，改用回归模型  
    若某些自变量的类别种类较多，或者自变量是区间型时，决策树过拟合的危险会增大。这种情况需要分箱或多次模型验证，确保其具有稳定性。  
    对区间型变量进行分箱操作时，无论是否考虑了顺序因素，都有可能因为分箱丧失了某些重要信息，尤其是当分箱前的区间型便变量与目标变量有明显的线性关系时，这种分箱造成的损失更为明显。

  
## CNN和LSTM原理和应用场景介绍  
    1）CNN原理：  
        a. 在原始图片上滑动窗口，将取值乘以卷积核进行特征映射，然后作为神经网络的数据。卷积核实际上是利用了先验的知识，“图片中距离较近的像素才能提供信息，距离较远的像素关系不大”。通过卷积核对图片中的一些特征进行抽取，如垂直、水平等  
        b. pooling：取窗口内的max或者avg，丢弃信息较少的数值  
        c. padding：补全，避免图片越抽取越小  
    2）CNN应用：  
        a. 图片分类等与图片有关的问题（图像识别、图像标注、图像主题生成、物体标注、视频分类等） （利用CNN抽取图片特征的能力）  
        b. 自然语言处理（实体抽取、关系抽取、问答系统、机器翻译）（将词用词向量表示，因此变成二维结构数据）  
    3）LSTM原理：  
        a. RNN（Recurrent Neural Network）能够把上一个时间的信息记忆，缺点是如果相隔太远联系就很弱了  
        b. LSTM（Long-Short Term Memory）在RNN的神经元中加入了一些组件，控制长短期的记忆。组件包括：  
            (1) 输入层门：将新的信息记录到细胞状态中  
            (2) 输出层门：将前面的信息保存到隐藏层中  
            (3) 忘记门：将细胞中的信息选择性遗忘（他今天有事，所以我。。。当处理到‘’我‘’的时候选择性的忘记前面的’他’，或者说减小这个词对后面词的作用。）（参考：https://blog.csdn.net/Dark_Scope/article/details/47056361；https://blog.csdn.net/roslei/article/details/61912618）  
    4）LSTM应用：  
        a. 自然语言类：机器翻译、在线问答、情感分析  
        b. 图片类：手写文字、图片内容理解  
        c. 音频类：语音识别  


## XGBOOST  
    xgb也是一种梯度提升树，是gbdt高效实现，差异是：  
    1）gbdt优化时只用到了一阶导数信息，xgb对代价函数做了二阶泰勒展开。（为什么使用二阶泰勒展开？我这里认为是使精度更高收敛速度更快，参考李宏毅的《机器学习》课程，对损失函数使用泰勒一次展开是梯度下降，而进行更多次展开能有更高的精度。但感觉还不完全正确，比如为什么不三次四次，比如引进二次导会不会带来计算开销的增加，欢迎大家讨论指正。）  
    2）xgb加入了正则项  
    3）xgb运行完一次迭代后，会对叶子节点的权重乘上shrinkage（缩减）系数，削弱当前树的影响，让后面有更大的学习空间  
    4）支持列抽样等特性  
    5）支持并行：决策树中对特征值进行排序以选择分割点是耗时操作，xgb训练之前就先对数据进行排序，保存为block结构，后续迭代中重复用该结构，大大减少计算量。同时各个特征增益的计算也可以开多线程进行  
    6）寻找最佳分割点时，实现了一种近似贪心法，同时优化了对稀疏数据、缺失值的处理，提高了算法效率  
    7）剪枝：GBDT遇到负损失时回停止分裂，是贪心算法。xgb会分裂到指定最大深度，然后再剪枝  


 
## GBDT和xgboost，bagging和boosting  
    12.1 GBDT  
    1）首先介绍Adaboost Tree，是一种boosting的树集成方法。基本思路是依次训练多棵树，每棵树训练时对分错的样本进行加权。树模型中对样本的加权实际是对样本采样几率的加权，在进行有放回抽样时，分错的样本更有可能被抽到  
    2）GBDT是Adaboost Tree的改进，每棵树都是CART（分类回归树），树在叶节点输出的是一个数值，分类误差就是真实值减去叶节点的输出值，得到残差。GBDT要做的就是使用梯度下降的方法减少分类误差值  
    在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。  
    GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。（参考：https://www.cnblogs.com/pinard/p/6140514.html）  
    3）得到多棵树后，根据每颗树的分类误差进行加权投票  
  
    12.2 xgboost  
    xgb也是一种梯度提升树，是gbdt高效实现，差异是：  
    1）gbdt优化时只用到了一阶导数信息，xgb对代价函数做了二阶泰勒展开。（为什么使用二阶泰勒展开？我这里认为是使精度更高收敛速度更快，参考李宏毅的《机器学习》课程，对损失函数使用泰勒一次展开是梯度下降，而进行更多次展开能有更高的精度。但感觉还不完全正确，比如为什么不三次四次，比如引进二次导会不会带来计算开销的增加，欢迎大家讨论指正。）  
    2）xgb加入了正则项  
    3）xgb运行完一次迭代后，会对叶子节点的权重乘上shrinkage（缩减）系数，削弱当前树的影响，让后面有更大的学习空间  
    4）支持列抽样等特性  
    5）支持并行：决策树中对特征值进行排序以选择分割点是耗时操作，xgb训练之前就先对数据进行排序，保存为block结构，后续迭代中重复用该结构，大大减少计算量。同时各个特征增益的计算也可以开多线程进行  
    6）寻找最佳分割点时，实现了一种近似贪心法，同时优化了对稀疏数据、缺失值的处理，提高了算法效率  
    7）剪枝：GBDT遇到负损失时回停止分裂，是贪心算法。xgb会分裂到指定最大深度，然后再剪枝  
  
    12.3 bagging  
    1）是一种自举聚合的方法，随机有放回地从样本内抽样构造分类器，然后多个分类器投票得到最终结果  
    2）可以降低方差，用于减少过拟合  
    3）常见的随机森林是bagging方法的应用  
    4）是并行的  
    5）最终投票一般是一个分类器一票  
  
    12.4 boosting  
    1）是一种将弱分类器组合起来形成强分类器的框架，串行结构，后一个分类器根据前一个分类器得到的信息进行重新训练，不断推进得到更好的模型  
    2）常见的boost方法有：  
        a. Adaboost：对每一次分类错误的样本进行加权，让下一个分类器更关心这些分错的样本  
        b. gbdt：每一个分类器都是cart树，输出的是分为正类的score。真实值减去score得到残差，下一棵树对残差进行训练。通过这种方法不断缩小对真实值差距  
    3）可以降低偏差，提高模型的表达能力，减少欠拟合  
    4）常见的有Adaboost和GBDT等  
    5）是串行的  
    6）一般是按照每个分类器的分类正确率进行加权投票  

## tree-base模型是否了解，决策树和随机森林区别是什么，树节点的分裂都有哪些策略（即节点的选择）

直接说随机森林是很多棵决策树组成的，一定程度上能避免过拟合问题。
树节点分裂：ID3 ： 以信息增益大小来建立。 
- C4.5 ：　以信息增益率大小来建立。 
- CART　：　以基尼系数大小来建立。
决策树的数据最好是离散型的吧？
ID3和C4.5是分类型决策树，只处理离散型的，CART是分类回归都可以做的决策树，支持所有类型




PART FOUR: 特征工程, 降维与其它
==========================

## 特征选择的方法  
    1）过滤：计算特征与标签之间的卡方、互信息、相关系数（只能识别线性关系），过滤掉取值较低的特征。或者使用树模型建模，通过树模型的importance进行选择（包括包外样本检验平均不纯度、特征使用次数等方法）  
    2）包裹：认为特征间的交叉也包含重要信息，因此计算特征子集的效果  
    3）嵌入法：L1正则化可以将不重要的特征降到0、树模型抽取特征  
    4）降维：PCA、LDA等  


## PCA  
    1）主成分分析是一种降维的方法  
    2）思想是将样本从原来的特征空间转化到新的特征空间，并且样本在新特征空间坐标轴上的投影方差尽可能大，这样就能涵盖样本最主要的信息  
    3）方法：  
        a. 特征归一化  
        b. 求样本特征的协方差矩阵A  
        c. 求A的特征值和特征向量，即AX=λX  
     d. 将特征值从大到小排列, 选择topK, 对应的特征向量就是新的坐标轴（采用最大方差理论解释，[参考](https://blog.csdn.net/huang1024rui/article/details/46662195)）  
    4）PCA也可以看成激活函数为线性函数的自动编码机（参考林轩田的《机器学习基石》第13课，深度学习）  

## 梯度下降和极大似然  
    1）梯度下降：  
        a. 是解决优化问题的一种方法，较适合于凸函数的优化，可以找到极值（极小值和极大值）  
        b. 对于某个参数，计算损失函数对该参数的偏导，该偏导即为下降方向。然后参数沿着该方向更新一个步长（学习率）  
  
    c. 迭代直到满足迭代次数或者参数不再变化  
    d. 包括梯度下降、随机梯度下降、mini-batch梯度下降  
    e. 只用到了一阶导信息，用牛顿法可以引入二阶导数信息  
    f. 梯度下降有效性的证明：用泰勒展开看  
（[参考@杨涛 的回答](https://www.zhihu.com/question/24258023) ）  
  
    2）极大似然估计：  
        a. 思想：事件概率A与一个参数θ有关，我们观察到一系列事件，那么此时θ的取值应该是能使P(A|θ)最大的那个值。  
        b. 过程：  
            (1)写出似然函数  
  
            (2)我们求解的目标是使似然函数最大  
  
            (3)因为是乘法问题，一般log化变成加法问题求解。即对要求的参数θ求偏导，令其为0  
  
（[参考](https://blog.csdn.net/zengxiantao1994/article/details/72787849)）  

## 维度怎么扩展？
比如时间，那我可能取分段区间。可能单取年份，可能取距离当今多久

## 特征怎么选择？
确定方差最大化，或者通过分类，寻找特征差异最大化。其他就是有具体看业务，要跟业务相结合，有统计性的（比如用tfidf转换行为轨迹），有直接性的（教育、性别等）

## 哪些机器学习算法不需要做归一化处理
概率模型不需要归一化，
因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。  
而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。
之所以进行数据归一化是因为各维度的量纲不相同，同时作为变量的时候可能会造成数值计算的问题，比如说求矩阵的逆可能很不精确或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能量纲也需要调整  


## 特征离散化优点  
1.离散特征的增加和减少都很容易，易于模型的快速迭代  
2.稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展  
3.离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰  
4.逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合 
5.离散化后可以进行特征交叉，由M+N个变量变为MN个变量，进一步引入非线性，提升表达能力  
6.特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问  
7.特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险  
模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验   

  
## 请尝试向非技术人员阐释交叉验证  
将数据样本切割成较小的子集，一部分用于训练模型，一部分用于验证模型（训练集的规模比验证集的规模大得多），利用验证集来测试训练得到的模型,主要用于  
评估模型的性能。通过模型在训练集上的表现和在验证集上的表现差异，来评估模型的泛化能力，和最终确定模型    
常见的有：k-folds 交叉验证，leave-one-out法  
k-folds: 将初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次  
平均K次的结果或者使用其它结合方式，最终得到一个单一估测  


## 用于评估预测模型的矩阵称为什么
混淆矩阵（confusion matrix），其列代表预测的类别，行代表真实值的分类

## tf-idf的公式
*Term Frequency-Inverse Document Frequency(TF-IDF)*
TFIDF实际上是：TF * IDF
词频（term frequency，TF）指的是某一个给定的词语在该文件中出现的频率。
逆向文件频率（inverse document frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目。
这边的例子以上述的数学公式来计算。词频 (TF) 是一词语出现的次数除以该文件的总词语数。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。一个计算文件频率 (IDF) 的方法是文件集里包含的文件总数除以测定有多少份文件出现过“母牛”一词。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是 lg10,000,000 / 1,000)=4。最后的TF-IDF的分数为0.03 * 4=0.12。

