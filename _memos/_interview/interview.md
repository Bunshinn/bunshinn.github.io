面试题

[TOC]

[//]:#[PART1](https://blog.csdn.net/u013382288/article/details/80417681)
##1. 怎么做恶意刷单检测 

分类问题用机器学习方法建模解决，我想到的特征有：
    1）商家特征：商家历史销量、信用、产品类别、发货快递公司等
    2）用户行为特征：用户信用、下单量、转化率、下单路径、浏览店铺行为、支付账号
    3）环境特征（主要是避免机器刷单）：地区、ip、手机型号等
    4）异常检测：ip地址经常变动、经常清空cookie信息、账号近期交易成功率上升等
    5）评论文本检测：刷单的评论文本可能套路较为一致，计算与已标注评论文本的相似度作为特征
    6）图片相似度检测：同理，刷单可能重复利用图片进行评论

##2. 你系统的学习过机器学习算法吗？
略。

##3. 选个讲下原理吧 K-Means算法及改进，遇到异常值怎么办？评估算法的指标有哪些？
    1）k-means原理
    2）改进：
        a. kmeans++：初始随机点选择尽可能远，避免陷入局部解。方法是n+1个中心点选择时，对于离前n个点选择到的概率更大
        b. mini batch kmeans：每次只用一个子集做重入类并找到类心（提高训练速度）
        c. ISODATA：对于难以确定k的时候，使用该方法。思路是当类下的样本小时，剔除；类下样本数量多时，拆分
        d. kernel kmeans：kmeans用欧氏距离计算相似度，也可以使用kernel映射到高维空间再聚类
    3）遇到异常值
        a. 有条件的话使用密度聚类或者一些软聚类的方式先聚类, 剔除异常值。不过本来用kmeans就是为了快, 这么做有些南辕北辙了
        b. 局部异常因子LOF：如果点p的密度明显小于其邻域点的密度，那么点p可能是异常值（参考：https://blog.csdn.net/wangyibo0201/article/details/51705966）
        c. 多元高斯分布异常点检测
        d. 使用PCA或自动编码机进行异常点检测：使用降维后的维度作为新的特征空间，其降维结果可以认为剔除了异常值的影响（因为过程是保留使投影后方差最大的投影方向）
        e. isolation forest：基本思路是建立树模型，一个节点所在的树深度越低，说明将其从样本空间划分出去越容易，因此越可能是异常值。是一种无监督的方法，随机选择n个sumsampe，随机选择一个特征一个值。（参考：https://blog.csdn.net/u013709270/article/details/73436588）
        f. winsorize：对于简单的，可以对单一维度做上下截取
    4）评估聚类算法的指标：
        a. 外部法（基于有标注）：Jaccard系数、纯度
        b. 内部法（无标注）：内平方和WSS和外平方和BSS
        c. 此外还要考虑到算法的时间空间复杂度、聚类稳定性等

##4. 数据预处理过程有哪些？
    1）缺失值处理：删、插
    2）异常值处理
    3）特征转换：时间特征sin化表示
    4）标准化：最大最小标准化、z标准化等
    5）归一化：对于文本或评分特征，不同样本之间可能有整体上的差异，如a文本共20个词，b文本30000个词，b文本中各个维度上的频次都很可能远远高于a文本
    6）离散化：onehot、分箱等

##5. 随机森林原理？有哪些随机方法？
    1）随机森林原理：通过构造多个决策树，做bagging以提高泛化能力
    2）subsample（有放回抽样）、subfeature、低维空间投影（特征做组合，参考林轩田的《机器学习基石》）

##6. PCA
    1）主成分分析是一种降维的方法
    2）思想是将样本从原来的特征空间转化到新的特征空间，并且样本在新特征空间坐标轴上的投影方差尽可能大，这样就能涵盖样本最主要的信息
    3）方法：
        a. 特征归一化
        b. 求样本特征的协方差矩阵A
        c. 求A的特征值和特征向量，即AX=λX
     d. 将特征值从大到小排列, 选择topK, 对应的特征向量就是新的坐标轴（采用最大方差理论解释，参考：https://blog.csdn.net/huang1024rui/article/details/46662195）
    4）PCA也可以看成激活函数为线性函数的自动编码机（参考林轩田的《机器学习基石》第13课，深度学习）
 

7. 还有一些围绕着项目问的具体问题
略。

8. 参加过哪些活动？
略。

##9. hive？spark？sql？ nlp？
    1）Hive允许使用类SQL语句在hadoop集群上进行读、写、管理等操作
    2）Spark是一种与hadoop相似的开源集群计算环境，将数据集缓存在分布式内存中的计算平台，每轮迭代不需要读取磁盘的IO操作，从而答复降低了单轮迭代时间

##10. XGBOOST
    xgb也是一种梯度提升树，是gbdt高效实现，差异是：
    1）gbdt优化时只用到了一阶导数信息，xgb对代价函数做了二阶泰勒展开。（为什么使用二阶泰勒展开？我这里认为是使精度更高收敛速度更快，参考李宏毅的《机器学习》课程，对损失函数使用泰勒一次展开是梯度下降，而进行更多次展开能有更高的精度。但感觉还不完全正确，比如为什么不三次四次，比如引进二次导会不会带来计算开销的增加，欢迎大家讨论指正。）
    2）xgb加入了正则项
    3）xgb运行完一次迭代后，会对叶子节点的权重乘上shrinkage（缩减）系数，削弱当前树的影响，让后面有更大的学习空间
    4）支持列抽样等特性
    5）支持并行：决策树中对特征值进行排序以选择分割点是耗时操作，xgb训练之前就先对数据进行排序，保存为block结构，后续迭代中重复用该结构，大大减少计算量。同时各个特征增益的计算也可以开多线程进行
    6）寻找最佳分割点时，实现了一种近似贪心法，同时优化了对稀疏数据、缺失值的处理，提高了算法效率
    7）剪枝：GBDT遇到负损失时回停止分裂，是贪心算法。xgb会分裂到指定最大深度，然后再剪枝

##11. 还问了数据库，spark，爬虫（简历中有）
略。

12. 具体案例分析，关于京东商城销售的

略。

 

##13. Linux基本命令
    1）目录操作：ls、cd、mkdir、find、locate、whereis等
    2）文件操作：mv、cp、rm、touch、cat、more、less
    3）权限操作：chmod+rwx421
    4）账号操作：su、whoami、last、who、w、id、groups等
    5）查看系统：history、top
    6）关机重启：shutdown、reboot
    7）vim操作：i、w、w!、q、q!、wq等

##14. NVL函数
    1）是oracle的一个函数
    2）NVL( string1, replace_with)，如果string1为NULL，则NVL函数返回replace_with的值，否则返回原来的值

##15. LR
    1）用于分类问题的线性回归
    2）采用sigmoid对输出值进行01转换
    3）采用似然法求解
    4）手推
    5）优缺点局限性
    6）改进空间

##16. sql中null与''的区别
    1）null表示空，用is null判断
    2）''表示空字符串，用=''判断

##17. 数据库与数据仓库的区别
    1）简单理解下数据仓库是多个数据库以一种方式组织起来
    2）数据库强调范式，尽可能减少冗余
    3）数据仓库强调查询分析的速度，优化读取操作，主要目的是快速做大量数据的查询
    4）数据仓库定期写入新数据，但不覆盖原有数据，而是给数据加上时间戳标签
    5）数据库采用行存储，数据仓库一般采用列存储
    6）数据仓库的特征是面向主题、集成、相对稳定、反映历史变化，存储数历史数据；数据库是面向事务的，存储在线交易数据
    7）数据仓库的两个基本元素是维表和事实表，维是看待问题的角度，比如时间、部门等，事实表放着要查询的数据

18. 手写SQL
略。


##19. SQL的数据类型
    1）字符串：char、varchar、text
    2）二进制串：binary、varbinary
    3）布尔类型：boolean
    4）数值类型：integer、smallint、bigint、decimal、numeric、float、real、double
    5）时间类型：date、time、timestamp、interval

20. C的数据类型
    1）基本类型：
        a. 整数类型：char、unsigned char、signed char、int、unsigned int、short、unsigned short、long、unsigned long
        b. 浮点类型：float、double、long double
    2）void类型
    3）指针类型
    4）构造类型：数组、结构体struct、共用体union、枚举类型enum

##21. 分类算法性能的主要评价指标
    1）查准率、查全率、F1
    2）AUC
    3）LOSS
    4）Gain和Lift
    5）WOE和IV

##22. roc图
    1）以真阳（TP）为横轴，假阳为纵轴（FP），按照样本预测为真的概率排序，绘制曲线
    2）ROC曲线下的面积为AUC的值

##23. 查准率查全率
    1）查准率：TP/(TP+FP)
    2）查全率：TP/(TP+FN)

##24. 数据缺失怎么办
    1）删除样本或删除字段
    2）用中位数、平均值、众数等填充
    3）插补：同类均值插补、多重插补、极大似然估计
    4）用其它字段构建模型，预测该字段的值，从而填充缺失值（注意：如果该字段也是用于预测模型中作为特征，那么用其它字段建模填充缺失值的方式，并没有给最终的预测模型引入新信息）
    5）onehot，将缺失值也认为一种取值
    6）压缩感知及矩阵补全

##25. 内连接与外连接的区别
    1）内连接：左右表取匹配行
    2）外连接：分为左连接、右连接和全连接

##26. 欧式距离
    1）字段取值平方和取开根号
    2）表示m维空间中两个点的真实距离

##27. 普通统计分析方法与机器学习的区别

这里不清楚普通统计分析方法指的是什么。
如果是简单的统计分析指标做预测，那模型的表达能力是落后于机器学习的。
如果是指统计学方法，那么统计学关心的假设检验，机器学习关心的是建模，两者的评估不同。

28. BOSS面：关于京东的想法，哪里人，什么学校，多大了，想在京东获得什么，你能为京东提供什么，关于转正的解释，工作内容，拿到offer

略。

29. 先问了一个项目，然后问了工作意向，对工作是怎么看待的
略。

30. 问了一点Java很基础的东西，像set、list啥的
略。

31. 感觉一二面的面试官比较在意你会不会hive、sql
略。

##32. 怎么判断一个账号不安全不正常了，比如被盗号了，恶意刷单之类的
分类问题用机器学习方法建模解决，我想到的特征有：
    1）商家特征：商家历史销量、信用、产品类别、发货快递公司等
    2）用户行为特征：用户信用、下单量、转化率、下单路径、浏览店铺行为、支付账号
    3）环境特征（主要是避免机器刷单）：地区、ip、手机型号等
    4）异常检测：ip地址变动、经常清空cookie信息、账号近期交易成功率上升等
    5）评论文本检测：刷单的评论文本可能套路较为一致，计算与已标注评论文本的相似度作为特征
    6）图片相似度检测：同理，刷单可能重复利用图片进行评论

##33. 只是岗位名称一样，我一面问的都是围绕海量数据的推荐系统，二面就十几分钟，都是自己再说……感觉凉的不能再凉了
    1）基于内容  
    2）协同过滤  
    3）基于矩阵分解   
    4）基于图  
其它包括冷启动、评估方法等

##34. 项目写的是天池比赛,只是大概描述了一下,特征工程和模型的选择
    1）数据预处理
    2）时间特征处理（sin化等）
    3）连续特征处理（分箱等）
    4）类别特征处理（onehot等）
    5）交叉特征
    6）特征hash化
    7）gbdt构造特征
    8）tfidf等对文本（或类似文本）的特征处理
    9）统计特征
    10）embedding方法作用于样本
    11）聚类、SVD、PCA等
    12）NN抽取特征
    13）自动编码机抽取特征

##35. GBDT原理介绍下
    1）首先介绍Adaboost Tree，是一种boosting的树集成方法。基本思路是依次训练多棵树，每棵树训练时对分错的样本进行加权。树模型中对样本的加权实际是对样本采样几率的加权，在进行有放回抽样时，分错的样本更有可能被抽到
    2）GBDT是Adaboost Tree的改进，每棵树都是CART（分类回归树），树在叶节点输出的是一个数值，分类误差就是真实值减去叶节点的输出值，得到残差。GBDT要做的就是使用梯度下降的方法减少分类误差值
    在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。
GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。[参考：](https://www.cnblogs.com/pinard/p/6140514.html)
    3）得到多棵树后，根据每颗树的分类误差进行加权投票

##36. XGBoost原理介绍下
见前文。

37. 用滑动窗口是怎样构造特征的
文本和图像数据中，设置窗口大小与滑动步长，以窗口为片段抽取特征。

##38. 简单的介绍随机森林，以及一些细节
    1）随机森林原理：通过构造多个决策树，做bagging以提高泛化能力
    2）随机方法包括：subsample（有放回抽样）、subfeature、低维空间投影（特征做组合，参考林轩田的《机器学习基石》）
    3）有放回抽样，可以用包外样本做检验
    4）也可以用OOB做特征选择，思路：
        a. 如果一个特征有效，那么这个特征引入杂质会明显影响模型效果
        b. 引入杂质会影响分布，所以更好的方式是对特征中的取值进行洗牌，然后计算前后模型的差异
        c. 但是我们不想训练两个模型，可以利用OOB进行偷懒。把OOB中的数据该特征取值洗牌，然后扔进训练好的模型中，用输出的结果进行误差检验

##39. 一个网站销售额变低，你从哪几个方面去考量？
    1）首先要定位到现象真正发生的位置，到底是谁的销售额变低了？这里划分的维度有：  
        a. 用户（画像、来源地区、新老、渠道等）  
        b. 产品或栏目  
        c. 访问时段  
    2）定位到发生未知后，进行问题拆解，关注目标群体中哪个指标下降导致网站销售额下降：  
        a. 销售额=入站流量*下单率*客单价  
        b. 入站流量 = Σ各来源流量*转化率  
        c. 下单率 = 页面访问量*转化率  
        d. 客单价 = 商品数量*商品价格  
    3）确定问题源头后，对问题原因进行分析，如采用内外部框架：  
        a. 内部：网站改版、产品更新、广告投放  
        b. 外部：用户偏好变化、媒体新闻、经济坏境、竞品行为等  

##40. 还有用户流失的分析，新用户流失和老用户流失有什么不同？
    1）用户流失分析：  
        a. 两层模型：细分用户、产品、渠道，看到底是哪里用户流失了。注意由于是用户流失问题，所以这里细分用户时可以细分用户处在生命周期的哪个阶段。  
        b. 指标拆解：用户流失数量 = 该群体用户数量*流失率。拆解，看是因为到了这个阶段的用户数量多了（比如说大部分用户到了衰退期），还是这个用户群体的流失率比较高  
        c. 内外部分析：  
            a. 内部：新手上手难度大、收费不合理、产品服务出现重大问题、活动质量低、缺少留存手段、用户参与度低等  
            b. 外部：市场、竞争对手、社会环境、节假日等  
    2）新用户流失和老用户流失有什么不同：  
        a. 新用户流失：原因可能有非目标用户（刚性流失）、产品不满足需求（自然流失）、产品难以上手（受挫流失）和竞争产品影响（市场流失）。  
        新用户要考虑如何在较少的数据支撑下做流失用户识别，提前防止用户流失，并如何对有效的新用户进行挽回。  
        b. 老用户流失：原因可能有到达用户生命周期衰退期（自然流失）、过度拉升arpu导致低端用户驱逐（刚性流失）、社交蒸发难以满足前期用户需求（受挫流失）和竞争产品影响（市场流失）。  
        老用户有较多的数据，更容易进行流失用户识别，做好防止用户流失更重要。当用户流失后，要考虑用户生命周期剩余价值，是否需要进行挽回。 （参考@王玮 的回答：https://www.zhihu.com/question/26225801）

##41. 京东商城要打5-6线渠道，PPT上放什么怎么放？对接人是CXO
（我刚准备开口讲面试官让我先思考一下）
    1）根据到底是CXO再决定  
    2）重点是了解CXO在这个打渠道行为中的角色，CXO关心的业绩指标是什么，然后针对性地展示 为了达成这个业绩指标 所相关的数据

##42. GMV升了20%怎么分析
（我噼里啪啦分析了一通面试官笑嘻嘻地告诉我是数据错了，因为面试较紧张没有意识到这个问题，现在想想真是个大坑啊）  
    1）参考该面试者经验，应该先估算一下数字有没有问题  
    2）同样的套路：  
        a. 两层模型：进行用户群体、产品、渠道细分，发现到底是谁的GMV提升了  
        b. 指标拆解：将GMV拆解成乘法模型，如GMV=广告投放数量\*广告点击率\*产品浏览量\*放入购物车率\*交易成功率\*客单价，检查哪一步有显著变化导致了GMV上升  
        c. 内外部分析：  
        a. 内部：网站、产品、广告投放、活动等  
        b. 外部：套PEST等框架也行，或者直接分析也行，注意MEMC即可  
    这一题要注意，GMV流水包括取消的订单金额和退货/拒收的订单金额，还有一种原因是商家刷单然后退货，虽然GMV上去了，但是实际成交量并没有那么多。

##43. 怎么向小孩子解释正态分布
（随口追问了一句小孩子的智力水平，面试官说七八岁，能数数）
    1）拿出小朋友班级的成绩表，每隔2分统计一下人数（因为小学一年级大家成绩很接近），画出钟形。然后说这就是正态分布，大多数的人都集中在中间，只有少数特别好和不够好
    2）拿出隔壁班的成绩表，让小朋友自己画画看，发现也是这样的现象
    3）然后拿出班级的身高表，发现也是这个样子的
    4）大部分人之间是没有太大差别的，只有少数人特别好和不够好，这是生活里普遍看到的现象，这就是正态分布

##44. 有一份分析报告，周一已定好框架，周五给老板，因为种种原因没能按时完成，怎么办？
略。

[PART2]()

##1. 二叉树题目
略

2. 层序遍历算法题
    1）由顶向下逐层访问
    2）可以用队列存储树，每次打印根节点并将左右节点放进队列
（参考：https://www.cnblogs.com/masterlibin/p/5911298.html）

3. 图论中的最大团、连通分量，然后问图划分的算法
略

4. 如何判断社区活跃度（基于图），现在想着可能是根据连通分量吧
略。

##5. 给定相邻两个节点的相似度，怎么计算该点到其它点的相似度
    1）把这个问题看成多维尺度分析问题（MDS），那么实际上就是已知点之间的距离，构造一个空间Z，使得这个空间内点之间的距离尽可能保持接近。点在新空间Z中的向量化就是点的表示，然后点到点的距离就可以。
（MDS求解参考：https://blog.csdn.net/u010705209/article/details/53518772?utm_source=itdadao&utm_medium=referral）
    2）其它：已知节点间距离，将节点embedding。这里我不太懂，希望大家有思路的可以指点下，谢啦
    3）上诉两个答案也可能是我没看懂题意，因为该题的上下文是做复杂网络相关的研究。那么可能是知道任意两个相邻节点的相似度，求非相邻节点的相似度。这里可以参考simRank算法，即两个点的邻域越相似（有很多相似邻居），那么两个点越相似。有点像pageRank，是一个迭代的定义。（参考：https://blog.csdn.net/luo123n/article/details/50212207）

##6. 给一堆学生的成绩，将相同学生的所有成绩求平均值并排序，让我用我熟悉的语言，我就用了python的字典+sorted，面试官说不准用sort，然后问会别的排序，我就说了冒泡排序，原理我说了，然后问我还知道其他排序，答堆排序（其实我之前这方面复习了很多），之后问我有没有实现过（这个问题简直就是我的死角，就是没实现过，所以才想找个实习练练啊）
    1）python直接pandas下groupby studentID sort
    2）实现排序算法

##7. 问了我机器学习熟悉的算法，答svm，讲一下原理
    1）一种分类方法，找到一个分类的超平面，将正负例分离，并让分类间隔尽可能大  
    2）过程：  
        a. 线性svm：损失函数如下  

        b. 对偶学习问题  
        c. 核函数：为了实现非线性分类，可以将样本映射到高维平面，然后用超平面分割。为了减少高维平面计算内积的操作，可以用一些“偷吃步”的方法同时进行高维映射和内积计算，就是核函数。包括多项式核函数、高斯核函数和sigmoid核函数  
        d. soft kernel
（参考林轩田《机器学习技法》，SVM这部分的推导讲得很清楚；或者参考https://blog.csdn.net/abcjennifer/article/details/7849812/）

    3）优点：  
        a. 容易抓住特征和目标之间的非线性关系  
        b. 避免陷入局部解，泛化能力强  
        c. 可以解决小样本高维问题（如文本分类）  
        d. 分类时只用到了支持向量，泛化能力强  
    4）缺点：  
        a. 训练时的计算复杂度高  
        b. 核函数选择没有通用方案  
        c. 对缺失数据敏感  
 
##8. c中struct的对齐，我这个真的没听过，面试官让我之后自己查
    为了提高存储器的访问效率，避免读一个成员数据访问多次存储器，操作系统对基本数据类型的合法地址做了限制，要求某种类型对象的地址必须是某个值K的整数倍（K=2或4或8）
    1）Windows给出的对齐要求是:任何K（K=2或4或8）字节的基本对象的地址都必须是K的整数倍
    2）Linux的对齐要求是：2字节类型的数据（如short）的起始地址必须是2的整数倍，而较大（int \*,int double ,long）的数据类型的地址必须是4的整数倍
（参考：https://www.cnblogs.com/fengfenggirl/p/struct_align.html）

##9. 机器学习被调数据分析了，因为做推荐的，所以面试一直在聊具体场景的推荐方法，其他方面知识没有怎么问
略。

##10. 梯度下降和极大似然
    1）梯度下降：  
        a. 是解决优化问题的一种方法，较适合于凸函数的优化，可以找到极值（极小值和极大值）  
        b. 对于某个参数，计算损失函数对该参数的偏导，该偏导即为下降方向。然后参数沿着该方向更新一个步长（学习率）  

    c. 迭代直到满足迭代次数或者参数不再变化  
    d. 包括梯度下降、随机梯度下降、mini-batch梯度下降  
    e. 只用到了一阶导信息，用牛顿法可以引入二阶导数信息  
    f. 梯度下降有效性的证明：用泰勒展开看  
（参考：https://www.zhihu.com/question/24258023 @杨涛 的回答）

    2）极大似然估计：  
        a. 思想：事件概率A与一个参数θ有关，我们观察到一系列事件，那么此时θ的取值应该是能使P(A|θ)最大的那个值。  
        b. 过程：  
            (1)写出似然函数  

            (2)我们求解的目标是使似然函数最大  

            (3)因为是乘法问题，一般log化变成加法问题求解。即对要求的参数θ求偏导，令其为0  

（参考：https://blog.csdn.net/zengxiantao1994/article/details/72787849）

##11. 特征选择的方法
    1）过滤：计算特征与标签之间的卡方、互信息、相关系数（只能识别线性关系），过滤掉取值较低的特征。或者使用树模型建模，通过树模型的importance进行选择（包括包外样本检验平均不纯度、特征使用次数等方法）  
    2）包裹：认为特征间的交叉也包含重要信息，因此计算特征子集的效果  
    3）嵌入法：L1正则化可以将不重要的特征降到0、树模型抽取特征  
    4）降维：PCA、LDA等  

##12. GBDT和xgboost，bagging和boosting
    12.1 GBDT  
    1）首先介绍Adaboost Tree，是一种boosting的树集成方法。基本思路是依次训练多棵树，每棵树训练时对分错的样本进行加权。树模型中对样本的加权实际是对样本采样几率的加权，在进行有放回抽样时，分错的样本更有可能被抽到  
    2）GBDT是Adaboost Tree的改进，每棵树都是CART（分类回归树），树在叶节点输出的是一个数值，分类误差就是真实值减去叶节点的输出值，得到残差。GBDT要做的就是使用梯度下降的方法减少分类误差值  
    在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。  
    GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。（参考：https://www.cnblogs.com/pinard/p/6140514.html）  
    3）得到多棵树后，根据每颗树的分类误差进行加权投票

    12.2 xgboost  
    xgb也是一种梯度提升树，是gbdt高效实现，差异是：
    1）gbdt优化时只用到了一阶导数信息，xgb对代价函数做了二阶泰勒展开。（为什么使用二阶泰勒展开？我这里认为是使精度更高收敛速度更快，参考李宏毅的《机器学习》课程，对损失函数使用泰勒一次展开是梯度下降，而进行更多次展开能有更高的精度。但感觉还不完全正确，比如为什么不三次四次，比如引进二次导会不会带来计算开销的增加，欢迎大家讨论指正。）
    2）xgb加入了正则项
    3）xgb运行完一次迭代后，会对叶子节点的权重乘上shrinkage（缩减）系数，削弱当前树的影响，让后面有更大的学习空间
    4）支持列抽样等特性
    5）支持并行：决策树中对特征值进行排序以选择分割点是耗时操作，xgb训练之前就先对数据进行排序，保存为block结构，后续迭代中重复用该结构，大大减少计算量。同时各个特征增益的计算也可以开多线程进行
    6）寻找最佳分割点时，实现了一种近似贪心法，同时优化了对稀疏数据、缺失值的处理，提高了算法效率
    7）剪枝：GBDT遇到负损失时回停止分裂，是贪心算法。xgb会分裂到指定最大深度，然后再剪枝

    12.3 bagging  
    1）是一种自举聚合的方法，随机有放回地从样本内抽样构造分类器，然后多个分类器投票得到最终结果
    2）可以降低方差，用于减少过拟合
    3）常见的随机森林是bagging方法的应用
    4）是并行的
    5）最终投票一般是一个分类器一票

    12.4 boosting  
    1）是一种将弱分类器组合起来形成强分类器的框架，串行结构，后一个分类器根据前一个分类器得到的信息进行重新训练，不断推进得到更好的模型
    2）常见的boost方法有：
        a. Adaboost：对每一次分类错误的样本进行加权，让下一个分类器更关心这些分错的样本
        b. gbdt：每一个分类器都是cart树，输出的是分为正类的score。真实值减去score得到残差，下一棵树对残差进行训练。通过这种方法不断缩小对真实值差距
    3）可以降低偏差，提高模型的表达能力，减少欠拟合
    4）常见的有Adaboost和GBDT等
    5）是串行的
    6）一般是按照每个分类器的分类正确率进行加权投票

##13. 过拟合的解决办法
    1）增加数据   
    2）正则项  
    3）early stopping  
    4）控制模型复杂度：  
        a. dropout（我觉得类似于subfeature）  
        b. 剪枝、控制树深  
        c. 增大分割平面间隔  
    5）bagging  
    6）subsampe & subfeature  
    7）特征选择、特征降维  
    8）数据增强（加包含噪声的数据）  
    9）ensemble
（参考林轩田的《机器学习技法》）

##14. 50亿个url，找出指定的一个url
50亿个的话是哈希查找，考虑到数量比较大会有冲突问题，那么可以用布隆过滤器。缺点还是会有误判，把不属于该集合的认为属于。

##15. CNN和LSTM原理和应用场景介绍
    1）CNN原理：
        a. 在原始图片上滑动窗口，将取值乘以卷积核进行特征映射，然后作为神经网络的数据。卷积核实际上是利用了先验的知识，“图片中距离较近的像素才能提供信息，距离较远的像素关系不大”。通过卷积核对图片中的一些特征进行抽取，如垂直、水平等
        b. pooling：取窗口内的max或者avg，丢弃信息较少的数值
        c. padding：补全，避免图片越抽取越小
    2）CNN应用：
        a. 图片分类等与图片有关的问题（图像识别、图像标注、图像主题生成、物体标注、视频分类等） （利用CNN抽取图片特征的能力）
        b. 自然语言处理（实体抽取、关系抽取、问答系统、机器翻译）（将词用词向量表示，因此变成二维结构数据）
    3）LSTM原理：
        a. RNN（Recurrent Neural Network）能够把上一个时间的信息记忆，缺点是如果相隔太远联系就很弱了
        b. LSTM（Long-Short Term Memory）在RNN的神经元中加入了一些组件，控制长短期的记忆。组件包括：
            (1) 输入层门：将新的信息记录到细胞状态中
            (2) 输出层门：将前面的信息保存到隐藏层中
            (3) 忘记门：将细胞中的信息选择性遗忘（他今天有事，所以我。。。当处理到‘’我‘’的时候选择性的忘记前面的’他’，或者说减小这个词对后面词的作用。）（参考：https://blog.csdn.net/Dark_Scope/article/details/47056361；https://blog.csdn.net/roslei/article/details/61912618）
    4）LSTM应用：
        a. 自然语言类：机器翻译、在线问答、情感分析
        b. 图片类：手写文字、图片内容理解
        c. 音频类：语音识别

##16. 为什么正则化能处理过拟合
    1）惩罚了模型的复杂度，避免模型过度学习训练集，提高泛化能力
    2）剃刀原理：如果两个理论都能解释一件事情，那么较为简单的理论往往是正确的
    3）正则项降低了每一次系数w更新的步伐，使参数更小，模型更简单
    4）贝叶斯学派的观点，认为加入了先验分布（l1拉普拉斯分布，l2高斯分布），减少参数的选择空间

17. 腾讯视频和优酷的区别
略。

##18. 几大常用检验方法与比较
    1）两样本均值：t检验（样本量少）、u检验（样本量大）  
    2）多样本均值：F检验（方差齐性检验）、方差分析  
    3）两样本事件发生频数是否关联：卡方检验、秩和检验（有序多组多分类）、二项分布检验  
    4）序列自相关：DW检验、ADF检验  
    5）面板数据检验：F检验、H检验  
    6）相关性分析、回归分析这些算么？

##19. KMP算法
    1）目标是做字符串匹配，将时间复杂度从O(m\*n)降为O(m+n)  
    2）思想：利用了目标字符串内部的重复性，使比较时实现最大的移动量  
    3）方法：  
       a. 计算next[i]：表示字符串第1至i-1个字符的子串中，前缀后缀最长重复个数  
       b. 对比主串s和目标字符串ptr。当相等时，i和j都+1；当不相等时，j更新为next[j]。
（参考：https://www.zhihu.com/question/21923021 @逍遥行 的回答）

20. 哈夫曼编码
    1）一种编码方式，让出现次数越多的字符编码越短，从而压缩编码的长度
    2）过程：
        a. 建立一个加权树，出现次数越多的字符出现在越上层
        b. 左连接为0，右连接为1
        c. 到达字符的01组合记为该字符的编码
        d. 由于哈夫曼编码是前缀编码（如果没有一个编码是另一个编码的前缀，则称这样的编码为前缀编码。如0,101和100是前缀编码），因此可以唯一地还原
（参考：https://blog.csdn.net/xgf415/article/details/52628073，https://www.cnblogs.com/xidongyu/p/6031518.html）

##21. 给出一个商业业务例子，这个例子中使用模型会比数据查询和简单的统计分析更有效果
    1）推荐算法
    2）异常值检测
    3）精准营销
    4）信贷发放
    ......

22. 了不了解mapreduce
略。

##23. 数据库熟练程度
略。

##24. 平时看什么书
略。

##25. 偏差和方差
    1）偏差：预测值与真实值差异，偏差大表示欠拟合。然后引申到计算方式和解决方法  
    2）方差：预测值与均值的波动，方差大表示过拟合。然后引申到计算方式和解决方法

##26. 有一个类似抖音的app，请你设计推荐算法
    1）定义目标（评估函数）：点赞率、停留时间、下滑概率等  
    2）数据：  
        a. 用户属性  
        b. 视频属性  
        c. 用户-视频行为日志  
    3）方法：  
        a. 信息流产品的特征是内容更新快，因此如果采用协同过滤的话用基于人的协同过滤会比较合适  
        b. 如果还是接受不了基于人的协同过滤的更新速度（比如说用户增长特别快），对于分类问题建立机器学习模型，输出的是用户对每个视频点击可能性的score，本质上还是一个ctr预估问题  
        c. 特征包括：用户属性、视频属性、统计特征、时间特征、用户id-视频id交叉项、用户向量信息、视频向量信息（用SVD、word2vec、fm、基于图的随机游走都可以）、短期行为  
        d. 注意1：降低训练成本，如先粗略计算用户对视频的响应概率，只取头部做召回。然后对召回的视频再做排序等  
        e. 注意2：需要能在线学习，用户点击后能对视频进行快速重排序。因此需要支持在线学习的模型，如lr、摇臂老虎机框架等，树模型在这里可能就不适用了  
        f. 注意3：冷启动问题  
            (1)用户冷启动：用户进来时根据用户属性对热门的内容进行匹配，找到用户可能最感兴趣的内容  
            (2)内容冷启动：这里首先要涉及到视频信息的标注，除了上传者信息、视频标题和标签，还需要一些视频理解的算法，为视频打标签。然后计算视频之间的相似程度，找到这一类视频的高响应用户，一定概率向其投放，统计点击率情况，判断是否进一步扩大投放量。

##27. 一个线段上任意取两点，能组成三角形的概率
    1）设线段长度为1，三个子线段长度为x1,x2,x3  
    2）根据三角形两边之和大于第三边，可得：  
            a. x1+x2>x3  
            b. x1-x2<x3  
            c. x2-x1<x3  
            d. x1+x2+x3=1  
    3）将x3=1-x1-x2带入abc，然后用x1、x2为轴绘制，可以得到有效面积为1/8

##28. 有uid，app名称，app类别，数据百亿级别，设计算法算出每个app类别只安装了一个app的uid总数。
    应该用map reduce吧，但我不会啊。准备写个sql，结果写了半天还是写不出。面试完走到楼下就想出来了，233
    1）小数据量的话直接查询：  
        select count(*) as result from
        (select uid, category, count(app) as c from table
        group by category having c = 1) as t
        group by t.uid having count(category) = 类别数
    2）大数据量下（没用过hadoop不太清楚，望大家指正）  
        a. 原始文件可以拼接为uid-app-categroy  
        b. map阶段形成的<k,v>是<uid-category,1>  
        c. reduce阶段统计key为“uid-category”的count数量  
        d. 只保留count为1的数据  
        e. 剩下的数据量直接统计uid出现次数=category类别数的数据

##29. 有一个网页访问的数据，包含uid，ip地址，url，文章资料。设计算法预测用户性别
    1）分类问题用机器学习方法解（这里假设已经有部分用户的性别标签）  
    2）想到的特征有：  
        a. 文档主题词（top3）  
  b. 文档标题词（按照标题词在文档中出现的频率，取top3）（参考：https://blog.csdn.net/u013382288/article/details/80385814）   
        c. ip地址  
        d. url中参数（如网页搜索中的query）  
        e. 统计特征：访问数量、单页面平局访问时间、上网时间等

笔试题

1、对于过拟合有什么方法处理
    见上文。

2、冒泡排序
    1）时间复杂度O（n）到O（n²）
    2）稳定排序

3、排列组合
    略。

4、大数定律和切比雪夫不等式的式子
    方差越大，X落在区间外的概率越大，X的波动也就越大。

5、回归系数的计算
    略。

6、鞍点的Hessian矩阵是否正定
    半正定，含有0特征值。

7、快速排序的最佳状况
    O（nlogn）

8、对于svm，梯度消失怎么在图像上判定
    不懂。

9、超参不敏感
    见下文，有详细题目。

10、分层抽样的适用范围
    略。

11、贝叶斯公式
    略。

12、高数里的一些求导的知识
    略。

13、线性代数里的秩、克莱姆法则
    1）向量组中的秩，就是极大线性无关向量组中的向量个数
    2）我们可以认为一个矩阵的秩，是给矩阵按质量排序的依据。
    秩越高的矩阵内容越丰富，冗余信息越少。秩越低的矩阵废数据越多。
（知乎 @苗加加）

3）克莱姆法则是求解线性方程组的定理，详见：https://baike.baidu.com/item/%E5%85%8B%E8%8E%B1%E5%A7%86%E6%B3%95%E5%88%99/7211518?fr=aladdin；https://blog.csdn.net/yjf_victor/article/details/45065847

14、推导回归系数的过程
（参考：https://blog.csdn.net/marsjohn/article/details/54911788）

15、深度优先遍历
    1）图的深度优先遍历：
        a. 首先以一个未被访问过的顶点作为起始顶点，沿当前顶点的边走到未访问过的顶点；
        b. 当没有未访问过的顶点时，则回到上一个顶点，继续试探别的顶点，直到所有的顶点都被访问过
    2）二叉树的深度优先遍历：实际就是前序遍历


解答题：

1、解释机器学习中的偏差和方差，对不同的情况应该采取什么样的措施？

    见上文。

2、描述假设检验的过程
    1）设置原假设H0，备择假设H1（一般我们的研究假设是H1）
    2）选定检验方法
    3）计算观测到的数值分分布，如果实际观察发生的是小概率事件，并且超过显著性水平，那么认为可以排除原假设H0

3、如果微信有一个功能是用户的位置信息能够每隔1分钟上传一次数据库，那么怎么发挥它的作用？
略。

笔试题

1.深度学习，训练集误差不断变小，测试集误差变大，要怎么做（ACD）
A 数据增强 B 增加网络深度 C提前停止训练 D增加 dropout

2. 鞍点的Hessian矩阵是？
半正定。

3.快排的时间复杂度
O（nlogn）

4 哪个sigmoid函数梯度消失最快？是零点处导数最大的还是最小的？
零点处导数最大。

5. 5 7 0 9 2 3 1 4 做冒泡排序的交换次数？
16？

6. 哪种优化方法对超参数不敏感？（C）
SGD BGD Adadelta Momentum

1）SGD受到学习率α影响

2）BGD受到batch规模m影响

3）Adagrad的一大优势时可以避免手动调节学习率，比如设置初始的缺省学习率为0.01，然后就不管它，另其在学习的过程中自己变化。

为了避免削弱单调猛烈下降的减少学习率，Adadelta产生了1。Adadelta限制把历史梯度累积窗口限制到固定的尺寸w，而不是累加所有的梯度平方和

4）Momentum：也受到学习率α的影响
[二](https://blog.csdn.net/u013382288/article/details/80470316)

##三
1、海量日志数据，提取出某日访问百度次数最多的那个IP。
　　首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。同样可以采用映射的方法，比如模1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hash_map进行频率统计，然后再找出频率最大的几个）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。
　　或者如下阐述：
　　算法思想：分而治之+Hash
1.IP地址最多有2^32=4G种取值情况，所以不能完全加载到内存中处理；
2.可以考虑采用“分而治之”的思想，按照IP地址的Hash(IP)24值，把海量IP日志分别存储到1024个小文件中。这样，每个小文件最多包含4MB个IP地址；
3.对于每一个小文件，可以构建一个IP为key，出现次数为＆#118alue的Hash
 map，同时记录当前出现次数最多的那个IP地址；
4.可以得到1024个小文件中的出现次数最多的IP，再依据常规的排序算法得到总体上出现次数最多的IP；
2、搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。
　　假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。），请你统计最热门的10个查询串，要求使用的内存不能超过1G。
　　典型的Top K算法，还是在这篇文章里头有所阐述，
　　文中，给出的最终算法是：
　　第一步、先对这批海量数据预处理，在O（N）的时间内用Hash表完成统计（之前写成了排序，特此订正。July、2011.04.27）；
　　第二步、借助堆这个数据结构，找出Top K，时间复杂度为N‘logK。
　　即，借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比所以，我们最终的时间复杂度是：O（N）
 + N’*O（logK），（N为1000万，N’为300万）。ok，更多，详情，请参考原文。
　　或者：采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。
3、有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。
　　方案：顺序读文件中，对于每个词x，取hash(x)P00，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。
　　如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
　　对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并（类似与归并排序）的过程了。
4、有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。
　　还是典型的TOP K算法，解决方案如下：
　　方案1：
　　顺序读取10个文件，按照hash(query)的结果将query写入到另外10个文件（记为）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。
　　找一台内存在2G左右的机器，依次对用hash_map(query,query_count)来统计每个query出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件（记为）。
　　对这10个文件进行归并排序（内排序与外排序相结合）。
　　方案2：
　　一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。
　　方案3：
　　与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。
5、
给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？
　　方案1：可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。
　　遍历文件a，对每个url求取hash(url)00，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,…,a999）中。这样每个小文件的大约为300M。
　　遍历文件b，采取和a相同的方式将url分别存储到1000小文件（记为b0,b1,…,b999）。这样处理后，所有可能相同的url都在对应的小文件（a0vsb0,a1vsb1,…,a999vsb999）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。
　　求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。
　　方案2：如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom
 filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloomfilter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。
Bloom filter日后会在本BLOG内详细阐述。
6、在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。
　　方案1：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32
 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。
　　方案2：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。
7、腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？
　　与上第6题类似，我的第一反应时快速排序+二分查找。以下是其它更好的方法：
　　方案1：oo，申请512M的内存，一个bit位代表一个unsigned
 int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。
　　方案2：这个问题在《编程珠玑》里有很好的描述，大家可以参考下面的思路，探讨一下：
　　又因为2^32为40亿多，所以给定一个数可能在，也可能不在其中；
　　这里我们把40亿个数中的每一个用32位的二进制来表示
　　假设这40亿个数开始放在一个文件中。
　　然后将这40亿个数分成两类:
1.最高位为0
2.最高位为1
　　并将这两类分别写入到两个文件中，其中一个文件中数的个数<=20亿，而另一个>=20亿（这相当于折半了）；
　　与要查找的数的最高位比较并接着进入相应的文件再查找
　　再然后把这个文件为又分成两类:
1.次最高位为0
2.次最高位为1
　　并将这两类分别写入到两个文件中，其中一个文件中数的个数<=10亿，而另一个>=10亿（这相当于折半了）；
　　与要查找的数的次最高位比较并接着进入相应的文件再查找。
　　…….
　　以此类推，就可以找到了,而且时间复杂度为O(logn)，方案2完。
　　附：这里，再简单介绍下，位图方法：
　　使用位图法判断整形数组是否存在重复
　　判断集合中存在重复是常见编程任务之一，当集合中数据量比较大时我们通常希望少进行几次扫描，这时双重循环法就不可取了。
　　位图法比较适合于这种情况，它的做法是按照集合中最大元素max创建一个长度为max+1的新数组，然后再次扫描原数组，遇到几就给新数组的第几位置上1，如遇到5就给新数组的第六个元素置1，这样下次再遇到5想置位时发现新数组的第六个元素已经是1了，这说明这次的数据肯定和以前的数据存在着重复。这种给新数组初始化时置零其后置一的做法类似于位图的处理方法故称位图法。它的运算次数最坏的情况为2N。如果已知数组的最大值即能事先给新数组定长的话效率还能提高一倍。
　　欢迎，有更好的思路，或方法，共同交流。
8、怎么在海量数据中找出重复次数最多的一个？
　　方案1：先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。
9、上千万或上亿数据（有重复），统计其中出现次数最多的钱N个数据。
　　方案1：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。然后就是取出前N个出现次数最多的数据了，可以用第2题提到的堆机制完成。
10、一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。
　　方案1：这题是考虑时间效率。用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度）。然后是找出出现最频繁的前10个词，可以用堆来实现，前面的题中已经讲到了，时间复杂度是O(n*lg10)。所以总的时间复杂度，是O(n*le)与O(n*lg10)中较大的哪一个。
　　附、100w个数中找出最大的100个数。
　　方案1：在前面的题中，我们已经提到了，用一个含100个元素的最小堆完成。复杂度为O(100w*lg100)。
　　方案2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100w*100)。
　　方案3：采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为O(100w*100)。
[三](https://blog.csdn.net/smarthhl/article/details/50390321)

#四
##1.请说明随机森林较一般决策树稳定的几点原因

随机森林分类的过程就是对于每个随机产生的决策树分类器，输入特征向量，森林中每棵决策树对样本进行分类，根据每个决策树的权重得到最后的分类结果。即随机森林就是由多颗决策树形成的并且随机森林是并行计算多颗决策树。 
bagging的方法，多个树投票提高泛化能力 
bagging中引入随机（参数、样本、特征、空间映射），避免单棵树的过拟合，提高整体泛化能力

决策树缺点和注意事项： 
    决策树的最大缺点是原理中的贪心算法。因此它所做的选择只能是某种意义上的局部最优选择。 
    若目标变量是连续变量，那么决策树就不使用了，改用回归模型 
    若某些自变量的类别种类较多，或者自变量是区间型时，决策树过拟合的危险会增大。这种情况需要分箱或多次模型验证，确保其具有稳定性。 
    对区间型变量进行分箱操作时，无论是否考虑了顺序因素，都有可能因为分箱丧失了某些重要信息，尤其是当分箱前的区间型便变量与目标变量有明显的线性关系时，这种分箱造成的损失更为明显。

##2.什么是聚类分析？聚类算法有哪几种？请选择一种详细描述其计算原理和步骤

1）聚类分析是一种无监督的学习方法，根据一定条件将相对同质的样本归到一个类总（俗话说人以类聚，物以群分） 
 正式一点的：聚类是对点集进行考察并按照某种距离测度将他们聚成多个“簇”的过程。聚类的目标是使得同一簇内的点之间的距离较短，而不同簇中点之间的距离较大。 
2）聚类方法主要有： 
a. 层次聚类 
层次法（hierarchical methods），这种方法对给定的数据集进行层次似的分解，直到某种条件满足为止。。具体又可分为“自底向上”和“自顶向下”两种方案。
　　例如，在“自底向上”方案中，初始时每一个数据纪录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。 
经典算法为：CURE；采用抽样技术先对数据集D随机抽取样本，再采用分区技术对样本进行分区，然后对每个分区局部聚类，最后对局部聚类进行全局聚类。 
b. 划分聚类：（经典算法为kmeans） 
划分法（parTITIoning methods），给定一个有N个元组或者纪录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K《N。而且这K个分组满足下列条件：
　　（1） 每一个分组至少包含一个数据纪录；
　　（2）每一个数据纪录属于且仅属于一个分组（注意：这个要求在某些模糊聚类算法中可以放宽）；
　　对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准就是：同一分组中的记录越近越好，而不同分组中的纪录越远越好。 
c. 密度聚类 
基于密度的方法（density-based methods），基于密度的方法与其它方法的一个根本区别是：它不是基于各种各样的距离的，而是基于密度的。这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。 
经典算法：DBSCAN:DBSCAN算法是一种典型的基于密度的聚类算法，该算法采用空间索引技术来搜索对象的邻域，引入了“核心对象”和“密度可达”等概念，从核心对象出发，把所有密度可达的对象组成一个簇。 
这个方法的指导思想：只要一个区域中的点的密度大过某个阈值，就把它加到与之相近的聚类中去。 
d. 网格聚类 
基于网格的方法（grid-based methods），这种方法首先将数据空间划分成为有限个单元（cell）的网格结构，所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关的，它只与把数据空间分为多少个单元有关。 
经典算法：STING：利用网格单元保存数据统计信息，从而实现多分辨率的聚类 
e. 模型聚类：高斯混合模型 
基于模型的方法（model-based methods），基于模型的方法给每一个聚类假定一个模型，然后去寻找能够很好的满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。 
3）k-means比较好介绍，选k个点开始作为聚类中心，然后剩下的点根据距离划分到类中；找到新的类中心；重新分配点；迭代直到达到收敛条件或者迭代次数。 优点是快；缺点是要先指定k，同时对异常值很敏感。

##3.以下算法对缺失值敏感的模型包括：（AE）

A、Logistic Regression（逻辑回归） 
B、随机森林 
C、朴素贝叶斯 
D、C4.5 
E、SVM

逻辑回归（目标变量是二元变量）

建模数据量不能太少，目标变量中每个类别所对应的样本数量要足够充分，才能支持建模 
排除共线性问题（自变量间相关性很大） 
异常值会给模型带来很大干扰，要剔除。 
 逻辑回归不能处理缺失值，所以之前应对缺失值进行适当处理。

随机森林的优点： 
    可以处理高维数据，不同进行特征选择（特征子集是随机选择） 
    模型的泛化能力较强 
    训练模型时速度快，成并行化方式，即树之间相互独立 
    模型可以处理不平衡数据，平衡误差 
    最终训练结果，可以对特种额排序，选择比较重要的特征 
    随机森林有袋外数据（OOB），因此不需要单独划分交叉验证集 
    对缺失值、异常值不敏感 
    模型训练结果准确度高 
    相对Bagging能够收敛于更小的泛化误差

朴素贝叶斯的假设前提有两个第一个为：各特征彼此独立；第二个为且对被解释变量的影响一致，不能进行变量筛选 
朴素贝叶斯对缺失值不敏感它

C4.5决策树 
C4.5算法的优点是：产生的分类规则易于理解，不用做特征选择，准确率较高。 
C4.5算法的缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效  
C4.5对缺失值不敏感，因为它有应对缺失值的处理方案。 
SVM： 
最优分类面就是要求分类线不但能将两类正确分开(训练错误率为0),且使分类间隔最大。SVM考虑寻找一个满足分类要求的超平面,并且使训练集中的点距离分类面尽可能的远,也就是寻找一个分类面使它两侧的空白区域(margin)最大。 
C是惩罚因子，是一个由用户去指定的系数，表示对分错的点加入多少的惩罚，当C很大的时候，分错的点就会更少，但是过拟合的情况可能会比较严重，当C很小的时候，分错的点可能会很多，不过可能由此得到的模型也会不太正确。 
SVM的优点： 
可以解决小样本，高维和非线性问题。 
可以避免神经网络结构选择和局部极小点问题。 
SVM的缺点： 
对缺失数据敏感。 
对非线性问题没有通用解决方案，须谨慎选择不同Kernelfunction来处理。

##3.线性回归和逻辑回归的区别

线性回归针对的目标变量是区间型的，   逻辑回归针对的目标变量是类别型的 
线性回归模型的目标变量和自变量之间的关系假设是线性相关的 ，逻辑回归模型中的目标变量和自变量是非线性的 
线性回归中通常会用假设，对应于自变量x的某个值，目标变量y的观察值是服从正太分布的。逻辑回归中目标变量y是服从二项分布0和1或者多项分布的 
逻辑回归中不存在线性回归中常见的残差 
参数估值上，线性回归采用最小平方法，逻辑回归采用最大似染法。
[四](https://blog.csdn.net/nilhurui/article/details/81346332)

#五
##1. 贝叶斯公式复述并解释应用场景
   1）P（A|B) = P(B|A)\*P(A) / P(B)
   2）如搜索query纠错，设A为正确的词，B为输入的词，那么：
      a. P(A|B)表示输入词B实际为A的概率
      b. P(B|A)表示词A错输为B的概率，可以根据AB的相似度计算（如编辑距离）
      c. P(A)是词A出现的频率，统计获得
      d. P(B)对于所有候选的A都一样，所以可以省去

##2. 如何写SQL求出中位数平均数和众数（除了用count之外的方法）
   1）中位数：
方案1（没考虑到偶数个数的情况）：
set @m = (select count(\*)/2 from table)
select column from table order by column limit @m, 1

方案2（考虑偶数个数，中位数是中间两个数的平均）：
set @index = -1
select avg(table.column)
from
(select @index:=@index+1 as index, column
from table order by column) as t
where t.index in (floor(@index/2),ceiling(@index/2))
   2）平均数：select avg(distinct column) from table
   3）众数：select column, count(\*) from table group by column order by column desc limit 1(emmm，好像用到count了）

3. 学过的机器学习算法有哪些
略。

##4. 如何避免决策树过拟合
   1）限制树深
   2）剪枝
   3）限制叶节点数量
   4）正则化项
   5）增加数据
   6）bagging（subsample、subfeature、低维空间投影）
   7）数据增强（加入有杂质的数据）
   8）早停

##5. 朴素贝叶斯的理解
   1）推导（参考：https://www.cnblogs.com/pinard/p/6069267.html）
   2）理解：朴素贝叶斯是在已知一些先验概率的情况下，由果索因的一种方法
   3）其它：朴素的意思是假设了事件相互独立

##6. SVM的优点
   1）优点：
      a. 能应用于非线性可分的情况
      b. 最后分类时由支持向量决定，复杂度取决于支持向量的数目而不是样本空间的维度，避免了维度灾难
      c. 具有鲁棒性：因为只使用少量支持向量，抓住关键样本，剔除冗余样本
      d. 高维低样本下性能好，如文本分类
   2）缺点：
      a. 模型训练复杂度高
      b. 难以适应多分类问题
      c. 核函数选择没有较好的方法论

##7. Kmeans的原理
   1）初始化k个点
   2）根据距离点归入k个类中
   3）更新k个类的类中心
   4）重复（2）（3），直到收敛或达到迭代次数

8. 对拼多多有什么了解，为什么选择拼多多
略。

##9. 口答两个SQL题（一个跟留存率相关，一个要用到row number）
   1）留存率：略
   2）mysql中设置row number：
SET @row_number = 0; SELECT (@row_number:=@row_number + 1) AS num FROM table

##10. 业务场景题，如何分析次日留存率下降的问题
   业务问题关键是问对问题，然后才是拆解问题去解决。  
   1）两层模型：从用户画像、渠道、产品、行为环节等角度细分，明确到底是哪里的次日留存率下降了  
   2）指标拆解：次日留存率 = Σ 次日留存数 / 今日获客人数  
   3）原因分析：  
      1）内部：  
         a. 运营活动  
         b. 产品变动  
         c. 技术故障  
         d. 设计漏洞（如产生可以撸羊毛的设计）  
   2）外部：  
      a. 竞品  
      b. 用户偏好  
      c. 节假日  
      d. 社会事件（如产生舆论）

##11. 处理需求时的一般思路是什么，并举例
   1）明确需求，需求方的目的是什么
   2）拆解任务
   3）制定可执行方案
   4）推进
   5）验收

12. 为什么选择拼多多
略。

13. 用过拼多多下单没，感受如何
略。

14. 可以接受单休和加班么
略。

15. 为啥要选数据分析方向（我简历上写的是数据挖掘工程师。。。）
略。

16. 开始聊项目，深究项目，我研究生阶段的方向比较偏，所以面试的三分之二时间都是在给他讲项目，好在最后他终于听懂了，thx god、、、
略。

17. hadoop原理和mapreduce原理
   1）Hadoop原理：采用HDFS分布式存储文件，MapReduce分解计算，其它先略
   2）MapReduce原理：
      a. map阶段：读取HDFS中的文件，解析成<k,v>的形式，并对<k,v>进行分区（默认一个区），将相同k的value放在一个集合中
      b. reduce阶段：将map的输出copy到不同的reduce节点上，节点对map的输出进行合并、排序
（参考：https://www.cnblogs.com/ahu-lichang/p/6645074.html）

18. 还有啥问题要问的？于是我出于本能的问了一句“为啥不写代码！” 然后面试官说“时间不够了。。。。”
略。

##19.现有一个数据库表Tourists，记录了某个景点7月份每天来访游客的数量如下： id date visits 1 2017-07-01 100 …… 非常巧，id字段刚好等于日期里面的几号。现在请筛选出连续三天都有大于100天的日期。 上面例子的输出为： date 2017-07-01 ……

解：
select t1.date
from Tourists as t1, Tourists as t2, Tourists as t3
on t1.id = (t2.id+1) and t2.id = (t3.id+1)
where t1.visits >100 and t2.visits>100 and t3.visits>100

##20.在一张工资表salary里面，发现2017-07这个月的性别字段男m和女f写反了，请用一个Updae语句修复数据 例如表格数据是： id name gender salary month 1 A m 1000 2017-06 2 B f 1010 2017-06
解：
update salary
set gender = replace('mf', gender, '')

##21.现有A表，有21个列，第一列id，剩余列为特征字段，列名从d1-d20，共10W条数据！ 另外一个表B称为模式表，和A表结构一样，共5W条数据 请找到A表中的特征符合B表中模式的数据，并记录下相对应的id 有两种情况满足要求： 1 每个特征列都完全匹配的情况下。 2 最多有一个特征列不匹配，其他19个特征列都完全匹配，但哪个列不匹配未知
解：（这题不懂怎么解）
select A.id,
((case A.d1 when B.d1 then 1 else 0) +
(case A.d2 when B.d2 then 1 else 0) +
...) as count_match
from A left join B
on A.d1 = B.d1

##22.我们把用户对商品的评分用稀疏向量表示，保存在数据库表t里面： t的字段有：uid，goods_id，star uid是用户id；goodsid是商品id；star是用户对该商品的评分，值为1-5。 现在我们想要计算向量两两之间的内积，内积在这里的语义为：对于两个不同的用户，如果他们都对同样的一批商品打了分，那么对于这里面的每个人的分数乘起来，并对这些乘积求和。 例子，数据库表里有以下的数据： U0 g0 2 U0 g1 4 U1 g0 3 U1 g1 1 计算后的结果为： U0 U1 2*3+4*1=10 ……
解：
select uid1, uid2, sum(result) as dot
from
(select t1.uid as uid1, t2.uid as uid2, t1.goods_id, t1.star\*t2.star as result
from t as t1, t as t2
on t1.goods_id = t2.goods_id) as t
group by goods_id

23.微信取消关注分析，题目太长了，没记录
略。

##24. 统计教授多门课老师数量并输出每位老师教授课程数统计表
解：设表class中字段为id，teacher，course

1）统计教授多门课老师数量
select count(*) from class
group by teacher having count(*) > 1

2）输出每位老师教授课程数统计
select teacher, count(course) as count_course
from class
group by teacher

##25. 四个人选举出一个骑士，统计投票数，并输出真正的骑士名字
解：设表tabe中字段为id，knight，vote_knight
select knight from table
group by vote_knight
order by count(vote_knight) limit 1

##26. 员工表，宿舍表，部门表，统计出宿舍楼各部门人数表
解：设员工表为employee，字段为id，employee_name，belong_dormitory_id，belong_department_id；
宿舍表为dormitory，字段为id，dormitory_number；
部门表为department，字段为id，department_name
select dormitory_number, department_name, count(employee_name) as count_employee
from employee as e
left join dormitory as dor on e.belong_dormitory_id = dor.id
left join department as dep on e.belong_department_id = dep.id

##27. 给出一堆数和频数的表格，统计这一堆数中位数
解：设表table中字段为id,number,frequency
set @sum = (select sum(frequency)+1 as sum from table)
set @index = 0
set @last_index = 0
select avg(distinct t.frequecy)
from
(select @last_index := @index, @index := @index+frequency as index, frequency
from table) as t
where t.index in (floor(@sum/2), ceiling(@sum/2))
or (floor(@sum/2) > t.last_index and ceiling(@sum.2) <= t.index)

##28. 中位数，三个班级合在一起的一张成绩单，统计每个班级成绩中位数
解：设表table中字段为id，class，score
select t1.class, avg(distinct t1.score) as median
from table t1, table t2 on t1.id = t2.id
group by t1.class, t1.score
having sum(case when t1.score >= t2.score then 1else 0 end) >=
(select count(*)/2 from table where table.class = t1.class)
and
having sum(case when t1.score <= t2.score then 1else 0 end) >=
(select count(*)/2 from table where table.class = t1.class)

##29. 交易表结构为user_id,order_id,pay_time,order_amount
   写sql查询过去一个月付款用户量（提示 用户量需去重）最高的3天分别是哪几天
  写sql查询做昨天每个用户最后付款的订单ID及金额

1）select count(distinct user_id) as c from table group by month(pay_time) order by c desc limit 3

2）select order_id, order_amount from ((select user_id, max(pay_time) as mt from table group by user_id where DATEDIFF(pay_time, NOW()) = -1 as t1) left join table as t2 where t1.user_id = t2.user_id and t1.mt == t2.pay_time)

 

##30. PV表a(表结构为user_id,goods_id),点击表b(user_id,goods_id),数据量各为50万条，在防止数据倾斜的情况下，写一句sql找出两个表共同的user_id和相应的goods_id

select * from a
where a.user_id exsit (select user_id from b)
（这题不太懂，sql中如何防止数据倾斜）

##31. 表结构为user_id,reg_time,age, 写一句sql按user_id随机抽样2000个用户  写一句sql取出按各年龄段（每10岁一个分段，如（0,10））分别抽样1%的用户

1）随机抽样2000个用户
select * from table order by rand() limit 2000

2）取出各年龄段抽样1%的用户
set @target = 0
set @count_user = 0
select @target:=@target+10 as age_right, *
from table as t1
where t1.age >=@target-10 and t1.age < (@target)
and t1.id in
(select floor(count(*)*0.1） from table as t2
where t1.age >=@target-10 and t1.age < (@target)
order by rand() limit ??)

（mysql下按百分比取数没有想到比较好的方法，因为limit后面不能接变量。想到的方法是先计算出每个年龄段的总数，然后计算出1%是多少，接着给每一行加一个递增+1的行标，当行标=1%时，结束）

##32. 用户登录日志表为user_id,log_id,session_id,plat,visit_date 用sql查询近30天每天平均登录用户数量  用sql查询出近30天连续访问7天以上的用户数量

1）近三十天每天平均登录用户数量
select visit_date, count(distince user_id)
group by visit_date

2）近30天连续访问7天以上的用户数量

select t1.date
from table t1, table t2, ..., table t7
on t1.visit_date = (t2.visit_date+1) and t2.visit_date = (t3.visit_date+1)
and ... and t6.visit_date = (t7.visit_date+1）

##33. 表user_id,visit_date,page_name,plat  统计近7天每天到访的新用户数 统计每个访问渠道plat7天前的新用户的3日留存率和7日留存率

1）近7天每天到访的新用户数
select day(visit_date), count(distinct user_id)
from table
where user_id not in
(select user_id from table
where day(visit_date) < date_sub(visit_date, interval 7day))

2）每个渠道7天前用户的3日留存和7日留存
 三日留存

 先计算每个平台7日前的新用户数量
select t1.plat, t1.c/t2.c as retention_3
(select plat, count(distinct user_id)
from table
group by plat, user_id
having day(min(visit_date)) = date_sub(now(), interval 7 day)) as t1
left join
(select plat, count(distinct user_id) as c
from table
group by user_id having count(user_id) > 0
having day(min(visit_date)) = date_sub(now(), interval 7 day)
and day(max(visit_date)) > date_sub(now(), interval 7 day)
and day(max(visit_date)) <= date_sub(now(), interval 4day)) as t2
on t1.plat = t2.plat
[五](https://blog.csdn.net/u013382288/article/details/80450360)

#六[source](https://blog.csdn.net/Data_learning/article/details/81434426)
##1.设计一个估算配送时间的模型

   影响配送时间的因素：商家出餐速度、配送速度、用户交付速度（配送过程中，商家取餐与交付用户占到配送时长的一半以上。准确预测取餐和交付时间，可以减少骑手等待时间）（参考：https://blog.csdn.net/u013382288/article/details/78395989）
   1）商家出餐速度：品类、时段、天气、活动、销量等因素
   2）配送速度：时段、配送人力、配送产品数量、天气、节假日
   3）用户交付：楼层、时段、有没有电梯、星期几等
然后建模预估速度。

##七
##1. 做自我介绍，着重介绍跟数据分析相关的经验，还有自己为什么要做数据分析
略。

##2. 如果次日用户留存率下降了 5%该怎么分析
    1）首先采用“两层模型”分析：对用户进行细分，包括新老、渠道、活动、画像等多个维度，然后分别计算每个维度下不同用户的次日留存率。通过这种方法定位到导致留存率下降的用户群体是谁
    2）对于目标群体次日留存下降问题，具体情况具体分析。具体分析可以采用“内部-外部”因素考虑，内部因素分为获客（渠道质量低、活动获取非目标用户）、满足需求（新功能改动引发某类用户不满）、提活手段（签到等提活手段没打成目标、产品自然使用周期低导致上次获得的大量用户短期内不需要再使用等）；外部因素采用PEST分析，政治（政策影响）、经济（短期内主要是竞争环境，如对竞争对手的活动）、社会（舆论压力、用户生活方式变化、消费心理变化、价值观变化等偏好变化）、技术（创新解决方案的出现、分销渠道变化等）

##3. 关于假设检验的问题，然而我并没有答上来，面试官说没关系
假设检验的基本原理是：全称命题不能证明但可以被证伪。
令我们研究假设的相反假设为原假设，认为我们研究假设的发生是小概率事件。
如果我们的观察值是研究假设，那么认为可以排除原假设，我们的研究假设并不是小概率事件。

4. 问了笔试中的题目为什么没做，现场做
略。

5. 对今日头条的看法
略。

6. 关于采样的问题
略。

##7. 卖玉米如何提高收益，价格提高多少才能获取最大收益
收益 = 单价*销售量，那么我们的策略是提高单位溢价或者提高销售规模。
提高单位溢价的方法：品牌打造获得长期溢价，但缺陷是需要大量前期营销投入；加工商品占据价值链更多环节，如熟玉米、玉米汁、玉米蛋白粉；重定位商品，如礼品化等；价格歧视，根据价格敏感度对不同用户采用不同定价。
销售量=流量*转化率，上述提高单位溢价的方法可能对流量产生影响，也可能对转化率产生影响。
那么 收益 = 单价*流量*转化率，短期内能规模化采用的应该是进行价格歧视，如不同时间、不同商圈的玉米价格不同，采取高定价，然后对价格敏感的用户提供优惠券等。

##8. 类比到头条的收益，头条放多少广告可以获得最大收益，不需要真的计算，只要有个思路就行
收益 = 出价*流量*点击率\*有效转化率，放广告的数量会在提高流量，但会降低匹配程度，因此降低点击率。最大收益是找到这个乘积的最大值，是一个有约束条件的最优化问题。
同时参考价格歧视方案，可以对不同的用户投放不同数量的广告。

9. 最后问头条的使用感受
略。

10. 为什么做数据分析
略。

11. 自己的优缺点
略。

##12. APP激活量的来源渠道很多，怎样对来源渠道变化大的进行预警
    1）如果渠道使用时间较长，认为渠道的app激活量满足一个分布，比较可能是正态分布。求平均值和标准差，对于今日数值与均值差大于3/2/1个标准差的渠道进行预警
    2）对于短期的新渠道，直接与均值进行对比。

##13. 用户刚进来APP的时候会选择属性，怎样在保证有完整用户信息的同时让用户流失减少
采用技术接受模型（TAM）来分析，影响用户接受选择属性这件事的主要因素有：
    1）感知有用性：
        a. 文案告知用户选择属性能给用户带来的好处
    2）感知易用性：
        a. 关联用户第三方账号（如微博），可以冷启动阶段匹配用户更有可能选择的属性，推荐用户选择
    b. 交互性做好
    3）使用者态度：用户对填写信息的态度
        a. 这里需要允许用户跳过，后续再提醒用户填写
        b. 告知用户填写的信息会受到很好的保护
    4）行为意图：用户使用APP的目的性，难以控制
    5）外部变量：如操作时间、操作环境等，这里难以控制

##14. 男生点击率增加，女生点击率增加，总体为何减少
因为男女的点击率可能有较大差异，同时低点击率群体的占比增大。
如原来男性20人，点击1人；女性100人，点击99人，总点击率100/120。
现在男性100人，点击6人；女性20人，点击20人，总点击率26/120。
即那个段子“A系中智商最低的人去读B，同时提高了A系和B系的平均智商。”

##15. 立方体每面抽掉一层非棱角部分，面积和体积的变化
看不懂题意。

##16. F检验是干嘛的
    1）F检验是检验两个正态分布的样本的方差是否存在显著差异
    2）也可以用于对多组样本之间比较
    3）计量中，F检验原假设所有自变量对因变量都没有影响，排除原假设说明至少有一个自变量对因变量有影响

##17. 如何识别作弊用户（爬虫程序， 或者渠道伪造的假用户）
分类问题可以用机器学习的方法去解决，下面是我目前想到的特征：
    1）渠道特征：渠道、渠道次日留存率、渠道流量以及各种比率特征
    2）环境特征：设备（一般伪造假用户的工作坊以低端机为主）、系统（刷量工作坊一般系统更新较慢）、wifi使用情况、使用时间、来源地区、ip是否进过黑名单
    3）用户行为特征：访问时长、访问页面、使用间隔、次日留存、活跃时间、页面跳转行为（假用户的行为要么过于一致，要么过于随机）、页面使用行为（正常用户对图片的点击也是有分布的，假用户的行为容易过于随机）
    4）异常特征：设备号异常（频繁重置idfa）、ip异常（异地访问）、行为异常（突然大量点击广告、点赞）、数据包不完整等

##18. 如何学习新知识? (思路大概就是利用什么渠道，怎么获取)
略

##19. 行存储和列存储的区别
    1）行存储：传统数据库的存储方式，同一张表内的数据放在一起，插入更新很快。缺点是每次查询即使只涉及几列，也要把所有数据读取
    2）列存储：OLAP等情况下，将数据按照列存储会更高效，每一列都可以成为索引，投影很高效。缺点是查询是选择完成时，需要对选择的列进行重新组装。
“当你的核心业务是 OLTP 时，一个行式数据库，再加上优化操作，可能是个最好的选择。
当你的核心业务是 OLAP 时，一个列式数据库，绝对是更好的选择”

（参考：https://blog.csdn.net/qq_26091271/article/details/51778675；https://www.zhihu.com/question/29380943）
[七](https://blog.csdn.net/u013382288/article/details/80390324)

##八

数据仓库是一个面向主题的、集成的、相对稳定的、反映历史变化的数据集合，用于支持管理决策。